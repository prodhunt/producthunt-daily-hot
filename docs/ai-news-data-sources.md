# AIèµ„è®¯æ•°æ®æºæ‰©å±•æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº†é™¤Product Huntä¹‹å¤–çš„å¤šç§AIå’Œç§‘æŠ€èµ„è®¯æ•°æ®æºï¼Œç”¨äºä¸°å¯ŒAIèµ„è®¯å†…å®¹æ”¶é›†ï¼Œæå‡ä¿¡æ¯è¦†ç›–é¢å’Œå†…å®¹è´¨é‡ã€‚

## æ•°æ®æºåˆ†ç±»

### 1. å®˜æ–¹APIæ•°æ®æº

#### GitHub Trending API
**æ•°æ®å†…å®¹**ï¼šæ¯æ—¥/æ¯å‘¨/æ¯æœˆçƒ­é—¨å¼€æºé¡¹ç›®
- **APIåœ°å€**ï¼š`https://api.github.com/search/repositories`
- **è´¹ç”¨**ï¼šå…è´¹ï¼ˆæœ‰é€Ÿç‡é™åˆ¶ï¼‰
- **æ•°æ®ç‰¹ç‚¹**ï¼š
  - AIé¡¹ç›®å æ¯”å¾ˆé«˜
  - å¯æŒ‰è¯­è¨€ã€æ—¶é—´ç­›é€‰
  - åŒ…å«é¡¹ç›®æè¿°ã€æ˜Ÿæ ‡æ•°ã€è¯­è¨€ç­‰
- **APIç¤ºä¾‹**ï¼š
```python
import requests

def get_github_trending_ai():
    url = "https://api.github.com/search/repositories"
    params = {
        "q": "machine learning OR artificial intelligence OR AI",
        "sort": "stars",
        "order": "desc",
        "per_page": 10,
        "created": ">2024-01-01"  # æœ€è¿‘é¡¹ç›®
    }
    response = requests.get(url, params=params)
    return response.json()
```

#### Hacker News API
**æ•°æ®å†…å®¹**ï¼šç§‘æŠ€æ–°é—»ã€æŠ€æœ¯è®¨è®ºçƒ­ç‚¹
- **APIåœ°å€**ï¼š`https://hacker-news.firebaseio.com/v0/`
- **è´¹ç”¨**ï¼šå®Œå…¨å…è´¹
- **æ•°æ®ç‰¹ç‚¹**ï¼š
  - é«˜è´¨é‡æŠ€æœ¯è®¨è®º
  - å®æ—¶çƒ­ç‚¹è¿½è¸ª
  - ç¤¾åŒºæŠ•ç¥¨æ’åº
- **APIç¤ºä¾‹**ï¼š
```python
def get_hacker_news_top():
    # è·å–çƒ­é—¨æ•…äº‹IDåˆ—è¡¨
    top_stories = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json").json()

    stories = []
    for story_id in top_stories[:10]:  # å–å‰10ä¸ª
        story = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json").json()
        if story and 'title' in story:
            stories.append(story)
    return stories
```

#### Reddit API
**æ•°æ®å†…å®¹**ï¼šAIç›¸å…³subredditè®¨è®º
- **ä¸»è¦subreddit**ï¼šr/MachineLearning, r/artificial, r/technology, r/singularity
- **APIåœ°å€**ï¼š`https://www.reddit.com/r/{subreddit}/hot.json`
- **è´¹ç”¨**ï¼šå…è´¹ï¼ˆéœ€æ³¨å†Œåº”ç”¨ï¼‰
- **APIç¤ºä¾‹**ï¼š
```python
def get_reddit_ai_posts():
    subreddits = ['MachineLearning', 'artificial', 'technology']
    posts = []

    for subreddit in subreddits:
        url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=10"
        headers = {'User-Agent': 'AI-News-Bot/1.0'}
        response = requests.get(url, headers=headers)
        data = response.json()
        posts.extend(data['data']['children'])

    return posts
```

### 2. å­¦æœ¯/ç ”ç©¶æ•°æ®æº

#### arXiv API
**æ•°æ®å†…å®¹**ï¼šæœ€æ–°AI/MLå­¦æœ¯è®ºæ–‡
- **APIåœ°å€**ï¼š`http://export.arxiv.org/api/query`
- **è´¹ç”¨**ï¼šå®Œå…¨å…è´¹
- **æ•°æ®ç‰¹ç‚¹**ï¼š
  - æœ€å‰æ²¿ç ”ç©¶æˆæœ
  - æ¯æ—¥æ›´æ–°
  - åŒ…å«æ‘˜è¦ã€ä½œè€…ã€åˆ†ç±»
- **APIç¤ºä¾‹**ï¼š
```python
import feedparser

def get_arxiv_ai_papers():
    url = "http://export.arxiv.org/api/query"
    params = {
        "search_query": "cat:cs.AI OR cat:cs.LG OR cat:cs.CL",
        "start": 0,
        "max_results": 10,
        "sortBy": "submittedDate",
        "sortOrder": "descending"
    }

    query_url = url + "?" + "&".join([f"{k}={v}" for k, v in params.items()])
    feed = feedparser.parse(query_url)
    return feed.entries
```

#### Papers With Code
**æ•°æ®å†…å®¹**ï¼šAIè®ºæ–‡+ä»£ç å®ç°
- **ç½‘ç«™**ï¼šhttps://paperswithcode.com/
- **ç‰¹ç‚¹**ï¼š
  - è®ºæ–‡ä¸ä»£ç ç»“åˆ
  - æ€§èƒ½æ’è¡Œæ¦œ
  - æ•°æ®é›†ä¿¡æ¯
- **è·å–æ–¹å¼**ï¼šç½‘é¡µçˆ¬å–æˆ–RSSè®¢é˜…

### 3. ç§‘æŠ€åª’ä½“RSSæº

#### ä¸»è¦RSSæºåˆ—è¡¨
```python
RSS_SOURCES = {
    'techcrunch': 'https://techcrunch.com/feed/',
    'venturebeat_ai': 'https://venturebeat.com/ai/feed/',
    'mit_tech_review': 'https://www.technologyreview.com/feed/',
    'ai_news': 'https://www.artificialintelligence-news.com/feed/',
    'wired_ai': 'https://www.wired.com/feed/tag/ai/latest/rss',
    'the_verge': 'https://www.theverge.com/rss/index.xml',
    'ars_technica': 'https://feeds.arstechnica.com/arstechnica/index'
}
```

#### RSSè§£æç¤ºä¾‹
```python
import feedparser
from datetime import datetime, timedelta

def get_rss_news(days_back=1):
    news_items = []
    cutoff_date = datetime.now() - timedelta(days=days_back)

    for source_name, rss_url in RSS_SOURCES.items():
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries:
                pub_date = datetime(*entry.published_parsed[:6])
                if pub_date > cutoff_date:
                    news_items.append({
                        'source': source_name,
                        'title': entry.title,
                        'link': entry.link,
                        'summary': entry.summary,
                        'published': pub_date
                    })
        except Exception as e:
            print(f"Error fetching {source_name}: {e}")

    return sorted(news_items, key=lambda x: x['published'], reverse=True)
```

### 4. å¼€å‘è€…ç¤¾åŒºæ•°æ®æº

#### Dev.to API
**æ•°æ®å†…å®¹**ï¼šå¼€å‘è€…æŠ€æœ¯æ–‡ç« 
- **APIåœ°å€**ï¼š`https://dev.to/api/articles`
- **è´¹ç”¨**ï¼šå…è´¹
- **APIç¤ºä¾‹**ï¼š
```python
def get_devto_ai_articles():
    url = "https://dev.to/api/articles"
    params = {
        "tag": "ai,machinelearning,artificialintelligence",
        "top": "7",  # æœ¬å‘¨çƒ­é—¨
        "per_page": 20
    }
    response = requests.get(url, params=params)
    return response.json()
```

#### Stack Overflow API
**æ•°æ®å†…å®¹**ï¼šæŠ€æœ¯é—®ç­”ã€è¶‹åŠ¿æ ‡ç­¾
- **APIåœ°å€**ï¼š`https://api.stackexchange.com/2.3/`
- **è´¹ç”¨**ï¼šå…è´¹ï¼ˆæœ‰é…é¢é™åˆ¶ï¼‰

### 5. æŠ•èµ„/èèµ„æ•°æ®æº

#### Crunchbase API
**æ•°æ®å†…å®¹**ï¼šåˆ›ä¸šå…¬å¸èèµ„ä¿¡æ¯
- **APIåœ°å€**ï¼š`https://api.crunchbase.com/api/v4/`
- **è´¹ç”¨**ï¼šä»˜è´¹API
- **æ•°æ®ç‰¹ç‚¹**ï¼š
  - æƒå¨æŠ•èµ„æ•°æ®
  - å…¬å¸è¯¦ç»†ä¿¡æ¯
  - èèµ„è½®æ¬¡è¿½è¸ª

#### AngelList (Wellfound) API
**æ•°æ®å†…å®¹**ï¼šåˆ›ä¸šå…¬å¸ã€æŠ•èµ„ä¿¡æ¯
- **ç‰¹ç‚¹**ï¼šæ—©æœŸé¡¹ç›®å¤š
- **è´¹ç”¨**ï¼šéƒ¨åˆ†å…è´¹

### 6. ç¤¾äº¤åª’ä½“æ•°æ®æº

#### Twitter API v2
**æ•°æ®å†…å®¹**ï¼šå®æ—¶AIè®¨è®ºã€KOLè§‚ç‚¹
- **APIåœ°å€**ï¼š`https://api.twitter.com/2/`
- **è´¹ç”¨**ï¼šåŸºç¡€ç‰ˆå…è´¹ï¼Œé«˜çº§åŠŸèƒ½ä»˜è´¹
- **APIç¤ºä¾‹**ï¼š
```python
import tweepy

def get_twitter_ai_trends(api_key, api_secret, access_token, access_token_secret):
    auth = tweepy.OAuthHandler(api_key, api_secret)
    auth.set_access_token(access_token, access_token_secret)
    api = tweepy.API(auth)

    # æœç´¢AIç›¸å…³æ¨æ–‡
    tweets = tweepy.Cursor(api.search_tweets,
                          q="artificial intelligence OR machine learning OR #AI -RT",
                          lang="en",
                          result_type="popular").items(50)

    return [tweet for tweet in tweets]
```

## æ¨èç»„åˆæ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šå…¨é¢è¦†ç›–å‹
**é€‚ç”¨åœºæ™¯**ï¼šç»¼åˆAIèµ„è®¯å¹³å°
```
Product Hunt (æ–°äº§å“) +
GitHub Trending (å¼€æºé¡¹ç›®) +
arXiv (å­¦æœ¯ç ”ç©¶) +
TechCrunch RSS (è¡Œä¸šæ–°é—») +
Reddit (ç¤¾åŒºè®¨è®º)
```

### æ–¹æ¡ˆ2ï¼šå¼€å‘è€…å¯¼å‘å‹
**é€‚ç”¨åœºæ™¯**ï¼šæŠ€æœ¯å¼€å‘è€…ç¤¾åŒº
```
Product Hunt +
GitHub Trending +
Hacker News +
Dev.to API +
Stack Overflow
```

### æ–¹æ¡ˆ3ï¼šæŠ•èµ„è§†è§’å‹
**é€‚ç”¨åœºæ™¯**ï¼šæŠ•èµ„æœºæ„ã€åˆ›ä¸šè€…
```
Product Hunt +
Crunchbase +
TechCrunch +
VentureBeat +
AngelList
```

### æ–¹æ¡ˆ4ï¼šå­¦æœ¯ç ”ç©¶å‹
**é€‚ç”¨åœºæ™¯**ï¼šç ”ç©¶æœºæ„ã€å­¦è€…
```
arXiv +
Papers With Code +
Reddit r/MachineLearning +
MIT Technology Review +
Google Scholar
```

## æ•°æ®æ•´åˆæ¶æ„

### å¤šæºæ•°æ®èšåˆç¤ºä¾‹
```python
class AINewsAggregator:
    def __init__(self):
        self.sources = {
            'product_hunt': self.get_product_hunt_data,
            'github_trending': self.get_github_trending,
            'arxiv_papers': self.get_arxiv_papers,
            'tech_news': self.get_rss_feeds,
            'reddit_discussions': self.get_reddit_posts
        }

    def collect_daily_news(self):
        all_news = {}
        for source_name, fetch_func in self.sources.items():
            try:
                data = fetch_func()
                all_news[source_name] = data
                print(f"âœ… {source_name}: {len(data)} items")
            except Exception as e:
                print(f"âŒ {source_name}: {e}")

        return self.aggregate_and_rank(all_news)

    def aggregate_and_rank(self, news_data):
        # æ•°æ®å»é‡ã€æ’åºã€åˆ†ç±»é€»è¾‘
        aggregated = []
        for source, items in news_data.items():
            for item in items:
                aggregated.append({
                    'source': source,
                    'title': item.get('title', ''),
                    'url': item.get('url', ''),
                    'score': self.calculate_score(item),
                    'category': self.categorize_content(item)
                })

        return sorted(aggregated, key=lambda x: x['score'], reverse=True)
```

### å†…å®¹åˆ†ç±»å»ºè®®
```python
CONTENT_CATEGORIES = {
    'ğŸš€ æ–°äº§å“å‘å¸ƒ': ['product_hunt'],
    'ğŸ’» å¼€æºé¡¹ç›®': ['github_trending'],
    'ğŸ“š å­¦æœ¯ç ”ç©¶': ['arxiv_papers', 'papers_with_code'],
    'ğŸ’° æŠ•èµ„èèµ„': ['crunchbase', 'angellist'],
    'ğŸ—ï¸ è¡Œä¸šæ–°é—»': ['techcrunch', 'venturebeat', 'mit_tech_review'],
    'ğŸ’¬ ç¤¾åŒºè®¨è®º': ['reddit', 'hacker_news', 'devto'],
    'ğŸ”¥ çƒ­é—¨è¯é¢˜': ['twitter', 'trending_topics']
}
```

## å®æ–½å»ºè®®

### æ›´æ–°é¢‘ç‡å»ºè®®
```python
UPDATE_SCHEDULE = {
    # æ¯æ—¥æ›´æ–°
    'daily': [
        'product_hunt',      # Product Huntæ¯æ—¥çƒ­æ¦œ
        'github_trending',   # GitHubæ¯æ—¥è¶‹åŠ¿
        'hacker_news',       # Hacker Newsçƒ­é—¨
        'reddit_hot'         # Redditçƒ­é—¨è®¨è®º
    ],

    # æ¯å‘¨æ±‡æ€»
    'weekly': [
        'arxiv_papers',      # å­¦æœ¯è®ºæ–‡å‘¨æŠ¥
        'investment_news',   # æŠ•èµ„èèµ„å‘¨æŠ¥
        'tech_analysis'      # æ·±åº¦æŠ€æœ¯åˆ†æ
    ],

    # å®æ—¶ç›‘æ§
    'realtime': [
        'twitter_trends',    # Twitterçƒ­ç‚¹
        'breaking_news',     # çªå‘æ–°é—»
        'market_alerts'      # å¸‚åœºåŠ¨æ€
    ]
}
```

### æ•°æ®è´¨é‡æ§åˆ¶
```python
class DataQualityFilter:
    def __init__(self):
        self.spam_keywords = ['spam', 'advertisement', 'promotion']
        self.min_engagement = {
            'github_stars': 10,
            'reddit_upvotes': 5,
            'twitter_likes': 10
        }

    def filter_content(self, items, source_type):
        filtered = []
        for item in items:
            if self.is_high_quality(item, source_type):
                filtered.append(item)
        return filtered

    def is_high_quality(self, item, source_type):
        # æ£€æŸ¥åƒåœ¾å†…å®¹
        title = item.get('title', '').lower()
        if any(keyword in title for keyword in self.spam_keywords):
            return False

        # æ£€æŸ¥å‚ä¸åº¦
        if source_type in self.min_engagement:
            engagement_key = {
                'github': 'stargazers_count',
                'reddit': 'ups',
                'twitter': 'favorite_count'
            }.get(source_type.split('_')[0])

            if engagement_key and item.get(engagement_key, 0) < self.min_engagement[source_type]:
                return False

        return True
```

## æ³¨æ„äº‹é¡¹

### 1. APIé™åˆ¶å’Œæˆæœ¬
```python
API_LIMITS = {
    'github': {
        'rate_limit': '5000/hour',
        'cost': 'Free',
        'auth_required': True
    },
    'twitter': {
        'rate_limit': '300/15min',
        'cost': 'Free tier available',
        'auth_required': True
    },
    'crunchbase': {
        'rate_limit': '200/day',
        'cost': '$29+/month',
        'auth_required': True
    },
    'reddit': {
        'rate_limit': '60/minute',
        'cost': 'Free',
        'auth_required': False
    }
}
```

### 2. æ³•å¾‹åˆè§„æ£€æŸ¥æ¸…å•
- [ ] **APIä½¿ç”¨æ¡æ¬¾**ï¼šä»”ç»†é˜…è¯»æ¯ä¸ªå¹³å°çš„ä½¿ç”¨æ¡æ¬¾
- [ ] **æ•°æ®ä½¿ç”¨æƒé™**ï¼šç¡®è®¤å•†ä¸šä½¿ç”¨æ˜¯å¦è¢«å…è®¸
- [ ] **ç‰ˆæƒå£°æ˜**ï¼šæ­£ç¡®æ ‡æ³¨å†…å®¹æ¥æº
- [ ] **ç”¨æˆ·éšç§**ï¼šé¿å…æ”¶é›†ä¸ªäººæ•æ„Ÿä¿¡æ¯
- [ ] **é¢‘ç‡é™åˆ¶**ï¼šéµå®ˆAPIè°ƒç”¨é¢‘ç‡é™åˆ¶
- [ ] **å†…å®¹è¿‡æ»¤**ï¼šè¿‡æ»¤ä¸å½“æˆ–æ•æ„Ÿå†…å®¹

### 3. æŠ€æœ¯å®ç°å»ºè®®
```python
# é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
import time
import random
from functools import wraps

def retry_on_failure(max_retries=3, delay=1):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    wait_time = delay * (2 ** attempt) + random.uniform(0, 1)
                    time.sleep(wait_time)
            return None
        return wrapper
    return decorator

# ç¼“å­˜æœºåˆ¶
import pickle
import os
from datetime import datetime, timedelta

class DataCache:
    def __init__(self, cache_dir='cache'):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def get(self, key, max_age_hours=1):
        cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
        if os.path.exists(cache_file):
            mod_time = datetime.fromtimestamp(os.path.getmtime(cache_file))
            if datetime.now() - mod_time < timedelta(hours=max_age_hours):
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
        return None

    def set(self, key, data):
        cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
        with open(cache_file, 'wb') as f:
            pickle.dump(data, f)
```

## æ‰©å±•æ•°æ®æº

### æ–°å…´å¹³å°
- **DiscordæœåŠ¡å™¨**ï¼šAIç¤¾åŒºè®¨è®º
- **Telegramé¢‘é“**ï¼šå®æ—¶AIèµ„è®¯
- **YouTube API**ï¼šAIæ•™ç¨‹å’Œæ¼”è®²
- **Podcast RSS**ï¼šAIæ’­å®¢å†…å®¹
- **Newsletter APIs**ï¼šä¸“ä¸šAIç®€æŠ¥

### å‚ç›´é¢†åŸŸæ•°æ®æº
```python
VERTICAL_SOURCES = {
    'computer_vision': [
        'https://www.pyimagesearch.com/feed/',
        'https://distill.pub/rss.xml'
    ],
    'nlp': [
        'https://ruder.io/rss/',
        'https://thegradient.pub/rss/'
    ],
    'robotics': [
        'https://spectrum.ieee.org/robotics/feed',
        'https://www.therobotreport.com/feed/'
    ],
    'autonomous_vehicles': [
        'https://www.thedrive.com/tech/rss',
        'https://electrek.co/guides/autonomous/feed/'
    ]
}
```

## ç›‘æ§å’Œåˆ†æ

### æ•°æ®æºè´¨é‡ç›‘æ§
```python
class SourceMonitor:
    def __init__(self):
        self.metrics = {}

    def track_source_performance(self, source_name, data):
        if source_name not in self.metrics:
            self.metrics[source_name] = {
                'total_items': 0,
                'quality_score': 0,
                'last_update': None,
                'error_count': 0
            }

        self.metrics[source_name]['total_items'] += len(data)
        self.metrics[source_name]['last_update'] = datetime.now()

    def get_source_health(self):
        health_report = {}
        for source, metrics in self.metrics.items():
            last_update = metrics['last_update']
            hours_since_update = (datetime.now() - last_update).total_seconds() / 3600

            health_report[source] = {
                'status': 'healthy' if hours_since_update < 24 else 'stale',
                'items_per_day': metrics['total_items'],
                'error_rate': metrics['error_count'] / max(1, metrics['total_items'])
            }

        return health_report
```

## æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†ä¸°å¯Œçš„AIèµ„è®¯æ•°æ®æºé€‰æ‹©ï¼Œä»å…è´¹çš„å¼€æºAPIåˆ°ä»˜è´¹çš„ä¸“ä¸šæœåŠ¡ï¼Œè¦†ç›–äº†å­¦æœ¯ç ”ç©¶ã€äº§å“å‘å¸ƒã€æŠ•èµ„èèµ„ã€æŠ€æœ¯è®¨è®ºç­‰å¤šä¸ªç»´åº¦ã€‚

### å¿«é€Ÿå¼€å§‹å»ºè®®
1. **ç¬¬ä¸€é˜¶æ®µ**ï¼šé›†æˆGitHub Trending + Hacker News + RSSæºï¼ˆå…è´¹ä¸”ç¨³å®šï¼‰
2. **ç¬¬äºŒé˜¶æ®µ**ï¼šæ·»åŠ Reddit + arXivï¼ˆæå‡å†…å®¹æ·±åº¦ï¼‰
3. **ç¬¬ä¸‰é˜¶æ®µ**ï¼šè€ƒè™‘ä»˜è´¹APIå¦‚Crunchbaseï¼ˆè·å–ç‹¬å®¶æ•°æ®ï¼‰

### æˆåŠŸå…³é”®å› ç´ 
- **æ•°æ®è´¨é‡**ï¼šå®æ–½ä¸¥æ ¼çš„å†…å®¹è¿‡æ»¤æœºåˆ¶
- **æ›´æ–°é¢‘ç‡**ï¼šä¿æŒå†…å®¹çš„æ—¶æ•ˆæ€§
- **ç”¨æˆ·ä½“éªŒ**ï¼šæä¾›æ¸…æ™°çš„åˆ†ç±»å’Œæœç´¢åŠŸèƒ½
- **åˆè§„è¿è¥**ï¼šéµå®ˆæ‰€æœ‰å¹³å°çš„ä½¿ç”¨æ¡æ¬¾

é€šè¿‡åˆç†ç»„åˆè¿™äº›æ•°æ®æºï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªå…¨é¢ã€åŠæ—¶ã€é«˜è´¨é‡çš„AIèµ„è®¯èšåˆå¹³å°ã€‚
```
