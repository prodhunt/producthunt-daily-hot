#!/usr/bin/env python3
"""
æ‰‹åŠ¨ç”ŸæˆæŒ‡å®šæ—¥æœŸçš„Product Huntå†…å®¹
ç”¨äºä¿®å¤ç¼ºå¤±æ•°æ®æˆ–é‡æ–°ç”Ÿæˆæœ‰é—®é¢˜çš„å†…å®¹
"""

import sys
import os
import json
import asyncio
import time
import argparse
from datetime import datetime, timedelta, timezone

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("dotenv æ¨¡å—æœªå®‰è£…ï¼Œå°†ç›´æ¥ä½¿ç”¨ç¯å¢ƒå˜é‡")

import requests
from bs4 import BeautifulSoup
import pytz
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from concurrent.futures import ThreadPoolExecutor

from scripts.llm_provider import get_llm_provider
from scripts.image_selector import ProductHuntImageSelector

llm = get_llm_provider()

class Product:
    def __init__(self, id: str, name: str, tagline: str, description: str, votesCount: int, createdAt: str, featuredAt: str, website: str, url: str, media=None, **kwargs):
        self.name = name
        self.tagline = tagline
        self.description = description
        self.votes_count = votesCount
        self.created_at = self.convert_to_beijing_time(createdAt)
        self.featured = "æ˜¯" if featuredAt else "å¦"
        self.website = website
        self.url = url
        self.og_image_url = self.get_image_url_from_media(media)
        self.keyword = self.generate_keywords()
        self.translated_tagline = self.translate_text(self.tagline)
        self.translated_description = self.translate_text(self.description)

    def get_image_url_from_media(self, media):
        try:
            if media and isinstance(media, list) and len(media) > 0:
                image_url = media[0].get('url', '')
                if image_url:
                    print(f"æˆåŠŸä»APIè·å–å›¾ç‰‡URL: {self.name}")
                    return image_url
            print(f"APIæœªè¿”å›å›¾ç‰‡ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {self.name}")
            backup_url = self.fetch_og_image_url()
            if backup_url:
                print(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è·å–å›¾ç‰‡URLæˆåŠŸ: {self.name}")
                return backup_url
            else:
                print(f"æ— æ³•è·å–å›¾ç‰‡URL: {self.name}")
            return ""
        except Exception as e:
            print(f"è·å–å›¾ç‰‡URLæ—¶å‡ºé”™: {self.name}, é”™è¯¯: {e}")
            return ""

    def fetch_og_image_url(self) -> str:
        try:
            response = requests.get(self.url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                og_image = soup.find("meta", property="og:image")
                if og_image:
                    return og_image["content"]
                twitter_image = soup.find("meta", name="twitter:image")
                if twitter_image:
                    return twitter_image["content"]
            return ""
        except Exception as e:
            print(f"è·å–OGå›¾ç‰‡URLæ—¶å‡ºé”™: {self.name}, é”™è¯¯: {e}")
            return ""

    def generate_keywords(self) -> str:
        try:
            print(f"æ­£åœ¨ä¸º {self.name} ç”Ÿæˆå…³é”®è¯...")
            keywords = llm.generate_keywords(self.name, self.tagline, self.description)
            if ',' not in keywords:
                keywords = ', '.join(keywords.split())
            print(f"æˆåŠŸä¸º {self.name} ç”Ÿæˆå…³é”®è¯")
            return keywords
        except Exception as e:
            print(f"å…³é”®è¯ç”Ÿæˆå¤±è´¥: {e}")
            words = set((self.name + ", " + self.tagline).replace("&", ",").replace("|", ",").replace("-", ",").split(","))
            return ", ".join([word.strip() for word in words if word.strip()])

    def translate_text(self, text: str) -> str:
        try:
            print(f"æ­£åœ¨ç¿»è¯‘ {self.name} çš„å†…å®¹...")
            translated_text = llm.translate_text(text)
            print(f"æˆåŠŸç¿»è¯‘ {self.name} çš„å†…å®¹")
            return translated_text
        except Exception as e:
            print(f"ç¿»è¯‘è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return text

    def convert_to_beijing_time(self, utc_time_str: str) -> str:
        utc_time = datetime.strptime(utc_time_str, '%Y-%m-%dT%H:%M:%SZ')
        beijing_tz = pytz.timezone('Asia/Shanghai')
        beijing_time = utc_time.replace(tzinfo=pytz.utc).astimezone(beijing_tz)
        return beijing_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %p%I:%M (åŒ—äº¬æ—¶é—´)')

    def to_markdown(self, rank: int) -> str:
        # ä¼˜åŒ–å›¾ç‰‡Altæ–‡æœ¬ï¼Œæä¾›æ›´è¯¦ç»†çš„SEOæè¿°
        seo_alt_text = f"{self.name} - {self.translated_tagline}ï¼Œè·å¾—{self.votes_count}ç¥¨"
        if self.featured == "æ˜¯":
            seo_alt_text += "ï¼ŒProduct Huntç²¾é€‰äº§å“"

        # ä¼˜åŒ–å›¾ç‰‡URLï¼Œæ¸…ç†ç°æœ‰å‚æ•°é¿å…é‡å¤
        if "?" in self.og_image_url:
            base_url = self.og_image_url.split("?")[0]
        else:
            base_url = self.og_image_url
        optimized_image_url = f"{base_url}?auto=format&w=800&h=400&fit=crop&q=85&fm=webp"
        og_image_markdown = f"![{seo_alt_text}]({optimized_image_url})"

        return (
            f"### {rank}. {self.name} - {self.translated_tagline}\n\n"
            f"**ä»‹ç»**ï¼š{self.translated_description}  \n"
            f"**å®˜æ–¹ç½‘ç«™**: [ç«‹å³è®¿é—®]({self.website})  \n\n"
            f"{og_image_markdown}\n\n"
            f"**ç¥¨æ•°**: ğŸ”º{self.votes_count}  \n"
            f"**æ˜¯å¦ç²¾é€‰**ï¼š{self.featured}  \n"
            f"**å‘å¸ƒæ—¶é—´**ï¼š{self.created_at}\n\n"
            f"---\n\n"
        )

def get_producthunt_token():
    developer_token = os.getenv('PRODUCTHUNT_DEVELOPER_TOKEN')
    if developer_token:
        print("ä½¿ç”¨ PRODUCTHUNT_DEVELOPER_TOKEN ç¯å¢ƒå˜é‡")
        return developer_token
    client_id = os.getenv('PRODUCTHUNT_CLIENT_ID')
    client_secret = os.getenv('PRODUCTHUNT_CLIENT_SECRET')
    if not client_id or not client_secret:
        raise Exception("Product Hunt client ID or client secret not found in environment variables")
    token_url = "https://api.producthunt.com/v2/oauth/token"
    payload = {
        "client_id": client_id,
        "client_secret": client_secret,
        "grant_type": "client_credentials"
    }
    try:
        response = requests.post(token_url, json=payload)
        response.raise_for_status()
        token_data = response.json()
        return token_data.get("access_token")
    except Exception as e:
        print(f"è·å– Product Hunt è®¿é—®ä»¤ç‰Œæ—¶å‡ºé”™: {e}")
        raise Exception(f"Failed to get Product Hunt access token: {e}")

def fetch_product_hunt_data_for_date(target_date):
    """è·å–æŒ‡å®šæ—¥æœŸçš„Product Huntæ•°æ®"""
    token = get_producthunt_token()
    date_str = target_date

    print(f"ğŸ¯ è·å– {date_str} çš„Product Huntæ•°æ®...")

    url = "https://api.producthunt.com/v2/api/graphql"
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": f"Bearer {token}",
        "User-Agent": "DecohackBot/1.0 (https://decohack.com)",
        "Origin": "https://decohack.com",
        "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
        "Connection": "keep-alive"
    }

    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session = requests.Session()
    session.mount("https://", adapter)

    base_query = """
    {
      posts(order: VOTES, postedAfter: "%sT00:00:00Z", postedBefore: "%sT23:59:59Z", after: "%s") {
        nodes {
          id
          name
          tagline
          description
          votesCount
          createdAt
          featuredAt
          website
          url
          media {
            url
            type
            videoUrl
          }
        }
        pageInfo {
          hasNextPage
          endCursor
        }
      }
    }
    """

    all_posts = []
    has_next_page = True
    cursor = ""

    while has_next_page and len(all_posts) < 10:
        query = base_query % (date_str, date_str, cursor)
        try:
            response = session.post(url, headers=headers, json={"query": query})
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
            raise Exception(f"Failed to fetch data from Product Hunt: {e}")

        data = response.json()['data']['posts']
        posts = data['nodes']
        all_posts.extend(posts)
        has_next_page = data['pageInfo']['hasNextPage']
        cursor = data['pageInfo']['endCursor']

    print(f"âœ… æˆåŠŸè·å– {len(all_posts)} ä¸ªäº§å“")
    return [Product(**post) for post in sorted(all_posts, key=lambda x: x['votesCount'], reverse=True)[:10]]

def generate_hugo_front_matter(products, date_str):
    """ç”ŸæˆHugo Front Matter"""
    try:
        # å‡†å¤‡äº§å“ä¿¡æ¯ç”¨äºæ ‡ç­¾ç”Ÿæˆ
        products_info = ""
        total_votes = 0

        for i, product in enumerate(products[:5], 1):  # åªä½¿ç”¨å‰5ä¸ªäº§å“
            total_votes += product.votes_count
            products_info += f"{i}. {product.name}\n"
            products_info += f"   - æ ‡è¯­: {product.tagline}\n"
            products_info += f"   - æè¿°: {product.description[:100]}...\n"
            products_info += f"   - ç¥¨æ•°: {product.votes_count}\n\n"

        # ç”Ÿæˆæ ‡ç­¾å’Œå…³é”®è¯
        print("ğŸ”„ æ­£åœ¨ç”ŸæˆHugoæ ‡ç­¾å’Œå…³é”®è¯...")
        tags_result = llm.generate_hugo_tags_and_keywords(products_info)

        # è§£æJSONç»“æœ
        try:
            tags_data = json.loads(tags_result)
            tags = tags_data.get('tags', ['Product Hunt', 'æ¯æ—¥çƒ­æ¦œ', 'åˆ›æ–°äº§å“'])
            keywords = tags_data.get('keywords', ['Product Hunt', 'PHçƒ­æ¦œ', 'ä»Šæ—¥æ–°å“'])
        except json.JSONDecodeError:
            print("âš ï¸ æ ‡ç­¾ç”Ÿæˆç»“æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾")
            tags = ['Product Hunt', 'æ¯æ—¥çƒ­æ¦œ', 'åˆ›æ–°äº§å“']
            keywords = ['Product Hunt', 'PHçƒ­æ¦œ', 'ä»Šæ—¥æ–°å“', 'åˆ›æ–°äº§å“æ¨è', 'ç§‘æŠ€äº§å“']

        # é€‰æ‹©å°é¢å›¾ç‰‡
        image_selector = ProductHuntImageSelector()
        # è½¬æ¢äº§å“æ•°æ®æ ¼å¼
        products_data = []
        for product in products:
            product_dict = {
                'name': product.name,
                'tagline': product.tagline,
                'votesCount': product.votes_count,
                'media': []
            }
            # å¦‚æœæœ‰å›¾ç‰‡URLï¼Œæ·»åŠ åˆ°mediaä¸­
            if hasattr(product, 'og_image_url') and product.og_image_url:
                product_dict['media'] = [{'url': product.og_image_url, 'type': 'image'}]
            products_data.append(product_dict)

        cover_url, alt_text = image_selector.select_best_cover_image(products_data)

        # ç”Ÿæˆä¼˜åŒ–çš„æ ‡é¢˜å’Œæè¿°
        top_product = products[0] if products else None
        featured_count = sum(1 for p in products if p.featured == "æ˜¯")
        ai_count = sum(1 for p in products if 'ai' in (p.name + p.tagline + p.description).lower())

        if top_product:
            # è®¡ç®—AIå·¥å…·å æ¯”
            ai_percentage = round((ai_count / len(products)) * 100) if products else 0

            # ä¼˜åŒ–æ ‡é¢˜ï¼ŒåŒ…å«æ›´å¤šSEOä¿¡æ¯
            if ai_percentage >= 50:
                title = f"Product Hunt ä»Šæ—¥çƒ­æ¦œ {date_str} | AIå·¥å…·å æ®{ai_percentage}%ä»½é¢ï¼Œ{top_product.name}{top_product.votes_count}ç¥¨é¢†è·‘"
            else:
                title = f"Product Hunt ä»Šæ—¥çƒ­æ¦œ {date_str} | {top_product.name}{top_product.votes_count}ç¥¨é¢†è·‘ï¼Œ{len(products)}æ¬¾åˆ›æ–°äº§å“"

            # ä¼˜åŒ–æè¿°ï¼Œæä¾›æ›´å¤šä»·å€¼ä¿¡æ¯
            second_product = products[1].name if len(products) > 1 else ""
            description = f"Product Hunt {date_str}çƒ­æ¦œæ·±åº¦åˆ†æï¼š{top_product.name}è·{top_product.votes_count}ç¥¨é¢†è·‘"
            if second_product:
                description += f"ï¼Œ{second_product}ç­‰"
            description += f"{len(products)}æ¬¾åˆ›æ–°äº§å“å®Œæ•´è§£æã€‚æ€»ç¥¨æ•°{total_votes}ç¥¨ï¼Œç²¾é€‰äº§å“{featured_count}æ¬¾"
            if ai_percentage >= 50:
                description += f"ï¼ŒAIå·¥å…·å æ®{ai_percentage}%ä»½é¢"
            description += "ã€‚"
        else:
            title = f"Product Hunt ä»Šæ—¥çƒ­æ¦œ {date_str}"
            description = f"ä»Šæ—¥Product Huntçƒ­æ¦œç²¾é€‰åˆ›æ–°äº§å“æ¨è"

        # ä¼˜åŒ–å°é¢å›¾ç‰‡URL
        if cover_url:
            # æ¸…ç†ç°æœ‰å‚æ•°ï¼Œé¿å…é‡å¤
            if "?" in cover_url:
                base_url = cover_url.split("?")[0]
            else:
                base_url = cover_url
            # æ·»åŠ SEOå‹å¥½çš„å›¾ç‰‡å‚æ•°
            cover_url = f"{base_url}?auto=format&w=1200&h=630&fit=crop&q=85&fm=webp"

        # æ„å»ºä¸“ä¸šSEOä¼˜åŒ–çš„Front Matter
        front_matter = "---\n"
        front_matter += f'title: "{title}"\n'
        front_matter += f'date: {date_str}T00:00:00+08:00\n'
        front_matter += f'lastmod: {date_str}T12:00:00+08:00\n'
        front_matter += f'description: "{description}"\n'
        front_matter += f'slug: "product-hunt-daily-{date_str}"\n'
        front_matter += f'categories: ["ç§‘æŠ€äº§å“", "Product Hunt"]\n'
        front_matter += f'tags: {json.dumps(tags, ensure_ascii=False)}\n'
        front_matter += f'keywords: {json.dumps(keywords, ensure_ascii=False)}\n'
        front_matter += f'author: "Product Hunt Daily"\n'

        # æ·»åŠ Hugo Stackæ”¯æŒçš„å°é¢å›¾ç‰‡å­—æ®µï¼ˆä½¿ç”¨æ­£ç¡®çš„coverç»“æ„ï¼‰
        if cover_url:
            front_matter += f'cover:\n'
            front_matter += f'  image: "{cover_url}"\n'
            front_matter += f'  alt: "Product Huntä»Šæ—¥çƒ­æ¦œï¼š{top_product.name if top_product else "åˆ›æ–°äº§å“"} - {top_product.translated_tagline if top_product and top_product.translated_tagline else "çƒ­é—¨äº§å“"} ({top_product.votes_count if top_product else 0}ç¥¨)"\n'

        # Hugo Stackä¸»é¢˜é…ç½®ï¼ˆåªä¿ç•™æ”¯æŒçš„å­—æ®µï¼‰
        front_matter += '\n# Hugo Stackä¸»é¢˜é…ç½®\n'
        front_matter += 'featured: true\n'
        front_matter += 'toc: true\n'
        front_matter += 'math: false\n'
        front_matter += 'lightgallery: true\n'
        front_matter += 'comments: true\n'
        front_matter += 'readingTime: true\n'
        front_matter += f'votes: {total_votes}\n'  # æ·»åŠ æ€»ç¥¨æ•°å­—æ®µ
        front_matter += "---\n\n"

        print("âœ… Hugo Front Matterç”ŸæˆæˆåŠŸ")
        return front_matter

    except Exception as e:
        print(f"âš ï¸ Hugo Front Matterç”Ÿæˆå¤±è´¥: {e}")
        # è¿”å›åŸºç¡€çš„Front Matter
        return f"""---
title: "Product Hunt ä»Šæ—¥çƒ­æ¦œ {date_str}"
date: {date_str}
description: "ä»Šæ—¥Product Huntçƒ­æ¦œç²¾é€‰åˆ›æ–°äº§å“æ¨è"
tags: ["Product Hunt", "æ¯æ—¥çƒ­æ¦œ", "åˆ›æ–°äº§å“"]
keywords: ["Product Hunt", "PHçƒ­æ¦œ", "ä»Šæ—¥æ–°å“", "åˆ›æ–°äº§å“æ¨è", "ç§‘æŠ€äº§å“"]
votes: {sum(p.votes_count for p in products) if products else 0}
---

"""

def generate_industry_analysis_content(products):
    """ç”Ÿæˆè¡Œä¸šåˆ†æå†…å®¹"""
    try:
        # å‡†å¤‡äº§å“ä¿¡æ¯ç”¨äºè¡Œä¸šåˆ†æ
        products_info = ""
        for i, product in enumerate(products[:10], 1):  # ä½¿ç”¨æ‰€æœ‰äº§å“è¿›è¡Œåˆ†æ
            products_info += f"{i}. {product.name}\n"
            products_info += f"   - æ ‡è¯­: {product.tagline}\n"
            products_info += f"   - æè¿°: {product.description[:100]}...\n"
            products_info += f"   - ç¥¨æ•°: {product.votes_count}\n"
            products_info += f"   - ç±»åˆ«: {categorize_product(product)}\n\n"

        print("ğŸ”„ æ­£åœ¨ç”Ÿæˆè¡Œä¸šè¶‹åŠ¿åˆ†æ...")
        industry_analysis = llm.generate_industry_analysis(products_info)
        print("âœ… è¡Œä¸šè¶‹åŠ¿åˆ†æç”ŸæˆæˆåŠŸ")

        return industry_analysis

    except Exception as e:
        print(f"âš ï¸ è¡Œä¸šåˆ†æç”Ÿæˆå¤±è´¥: {e}")
        return "## ğŸ” ä»Šæ—¥ç§‘æŠ€è¶‹åŠ¿åˆ†æ\n\nä»Šæ—¥Product Huntçƒ­æ¦œå±•ç°äº†ç§‘æŠ€äº§å“çš„å¤šå…ƒåŒ–å‘å±•è¶‹åŠ¿ï¼Œæ¶µç›–äººå·¥æ™ºèƒ½ã€ç”Ÿäº§åŠ›å·¥å…·ã€å¼€å‘è€…å·¥å…·ç­‰å¤šä¸ªé¢†åŸŸï¼Œåæ˜ äº†å½“å‰ç§‘æŠ€åˆ›æ–°çš„æ´»è·ƒæ€åŠ¿ã€‚"

def categorize_product(product):
    """ç®€å•çš„äº§å“åˆ†ç±»é€»è¾‘"""
    name_and_desc = (product.name + " " + product.tagline + " " + product.description).lower()

    if any(keyword in name_and_desc for keyword in ['ai', 'artificial intelligence', 'machine learning', 'ml', 'neural', 'gpt', 'llm']):
        return "AIå·¥å…·"
    elif any(keyword in name_and_desc for keyword in ['finance', 'money', 'investment', 'trading', 'stock', 'crypto', 'payment']):
        return "é‡‘èå·¥å…·"
    elif any(keyword in name_and_desc for keyword in ['code', 'developer', 'programming', 'api', 'github', 'software']):
        return "å¼€å‘å·¥å…·"
    elif any(keyword in name_and_desc for keyword in ['design', 'ui', 'ux', 'creative', 'visual', 'graphic']):
        return "è®¾è®¡å·¥å…·"
    elif any(keyword in name_and_desc for keyword in ['productivity', 'task', 'project', 'team', 'collaboration', 'workflow']):
        return "ç”Ÿäº§åŠ›å·¥å…·"
    elif any(keyword in name_and_desc for keyword in ['marketing', 'seo', 'social', 'analytics', 'campaign']):
        return "è¥é”€å·¥å…·"
    else:
        return "å…¶ä»–å·¥å…·"

def generate_markdown_for_date(products, date_str):
    """ä¸ºæŒ‡å®šæ—¥æœŸç”Ÿæˆmarkdownå†…å®¹"""
    # ç”ŸæˆHugo Front Matter
    front_matter = generate_hugo_front_matter(products, date_str)

    # ç”Ÿæˆè¡Œä¸šåˆ†æå†…å®¹
    industry_analysis = generate_industry_analysis_content(products)

    # ç”Ÿæˆä¼˜åŒ–çš„å†…å®¹ç»“æ„
    ai_count = sum(1 for p in products if 'ai' in (p.name + p.tagline + p.description).lower())
    ai_percentage = round((ai_count / len(products)) * 100) if products else 0
    top_product = products[0] if products else None

    # ç”Ÿæˆæ ‡é¢˜å’Œæè¿°ï¼ˆç”¨äºç»“æ„åŒ–æ•°æ®ï¼‰
    total_votes = sum(p.votes_count for p in products)
    featured_count = sum(1 for p in products if p.featured == "æ˜¯")

    if top_product:
        if ai_percentage >= 50:
            title = f"Product Hunt ä»Šæ—¥çƒ­æ¦œ {date_str} | AIå·¥å…·å æ®{ai_percentage}%ä»½é¢ï¼Œ{top_product.name}{top_product.votes_count}ç¥¨é¢†è·‘"
        else:
            title = f"Product Hunt ä»Šæ—¥çƒ­æ¦œ {date_str} | {top_product.name}{top_product.votes_count}ç¥¨é¢†è·‘ï¼Œ{len(products)}æ¬¾åˆ›æ–°äº§å“"

        second_product = products[1].name if len(products) > 1 else ""
        description = f"Product Hunt {date_str}çƒ­æ¦œæ·±åº¦åˆ†æï¼š{top_product.name}è·{top_product.votes_count}ç¥¨é¢†è·‘"
        if second_product:
            description += f"ï¼Œ{second_product}ç­‰"
        description += f"{len(products)}æ¬¾åˆ›æ–°äº§å“å®Œæ•´è§£æã€‚æ€»ç¥¨æ•°{total_votes}ç¥¨ï¼Œç²¾é€‰äº§å“{featured_count}æ¬¾"
        if ai_percentage >= 50:
            description += f"ï¼ŒAIå·¥å…·å æ®{ai_percentage}%ä»½é¢"
        description += "ã€‚"
    else:
        title = f"Product Hunt ä»Šæ—¥çƒ­æ¦œ {date_str}"
        description = f"ä»Šæ—¥Product Huntçƒ­æ¦œç²¾é€‰åˆ›æ–°äº§å“æ¨è"

    # è·å–å°é¢å›¾ç‰‡URL
    cover_url = None
    if products and hasattr(products[0], 'og_image_url') and products[0].og_image_url:
        # æ¸…ç†ç°æœ‰å‚æ•°ï¼Œé¿å…é‡å¤
        if "?" in products[0].og_image_url:
            base_url = products[0].og_image_url.split("?")[0]
        else:
            base_url = products[0].og_image_url
        cover_url = f"{base_url}?auto=format&w=1200&h=630&fit=crop&q=85&fm=webp"

    markdown_content = front_matter

    # æ·»åŠ å®Œæ•´çš„SEOæ ‡ç­¾ï¼ˆç›´æ¥HTMLè¾“å‡ºï¼Œä¸ä¾èµ–ä¸»é¢˜æ”¯æŒï¼‰
    # å…ˆå¤„ç†æ ‡é¢˜å’Œæè¿°ä¸­çš„å¼•å·
    safe_title = title.replace('"', '&quot;')
    safe_description = description.replace('"', '&quot;')

    # ç¤¾äº¤åª’ä½“æ ‡ç­¾ + ç»“æ„åŒ–æ•°æ®
    seo_tags = f'''<!-- SEOä¼˜åŒ–æ ‡ç­¾ -->
<meta property="og:title" content="{safe_title}">
<meta property="og:description" content="{safe_description}">
<meta property="og:type" content="article">
<meta property="og:site_name" content="Product Hunt æ¯æ—¥ä¸­æ–‡çƒ­æ¦œ">
<meta property="og:url" content="https://yourdomain.com/news/product-hunt-daily-{date_str}/">'''

    if cover_url:
        seo_tags += f'''
<meta property="og:image" content="{cover_url}">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="630">'''

    seo_tags += f'''

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Product Hunt ä»Šæ—¥çƒ­æ¦œ | {len(products)}æ¬¾åˆ›æ–°äº§å“æ¨è">
<meta name="twitter:description" content="{top_product.name if top_product else 'åˆ›æ–°äº§å“'}ç­‰çƒ­é—¨äº§å“æ¨è #ProductHunt #AI #ç§‘æŠ€">'''

    if cover_url:
        seo_tags += f'''
<meta name="twitter:image" content="{cover_url}">'''

    # JSON-LD ç»“æ„åŒ–æ•°æ®
    json_ld = f'''
<script type="application/ld+json">
{{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "{safe_title}",
  "description": "{safe_description}",
  "author": {{
    "@type": "Organization",
    "name": "Product Hunt Daily"
  }},
  "publisher": {{
    "@type": "Organization",
    "name": "Product Hunt Daily"
  }},
  "datePublished": "{date_str}T00:00:00+08:00",
  "dateModified": "{date_str}T12:00:00+08:00",
  "mainEntityOfPage": {{
    "@type": "WebPage",
    "@id": "https://yourdomain.com/news/product-hunt-daily-{date_str}/"
  }},
  "articleSection": "Technology",
  "wordCount": 2500'''

    if cover_url:
        json_ld += f''',
  "image": {{
    "@type": "ImageObject",
    "url": "{cover_url}",
    "width": 1200,
    "height": 630
  }}'''

    json_ld += '''
}
</script>'''

    markdown_content += seo_tags + json_ld + "\n\n"

    # ä¸»è¦å†…å®¹
    if ai_percentage >= 50:
        main_title = f"# Product Hunt ä»Šæ—¥çƒ­æ¦œ {date_str}ï¼šAIå·¥å…·å æ®ä¸»å¯¼åœ°ä½\n\n"
    else:
        main_title = f"# Product Hunt ä»Šæ—¥çƒ­æ¦œ {date_str}ï¼šåˆ›æ–°äº§å“ç²¾é€‰\n\n"

    markdown_content += main_title

    # ä»Šæ—¥äº®ç‚¹æ€»è§ˆ
    markdown_content += "## ğŸ“‹ ä»Šæ—¥äº®ç‚¹æ€»è§ˆ\n\n"
    markdown_content += "### ğŸ† çƒ­é—¨äº§å“æ¨è\n"

    # æ˜¾ç¤ºå‰3ä¸ªäº§å“
    for i, product in enumerate(products[:3], 1):
        stars = "â­" * min(5, max(1, product.votes_count // 100))
        markdown_content += f"- **[{product.name}](#{i}-{product.name.lower().replace(' ', '-')})** - {product.translated_tagline} ({product.votes_count}ç¥¨) {stars}\n"

    # æ•°æ®ç»Ÿè®¡
    hot_products_count = sum(1 for p in products if p.votes_count >= 200)
    markdown_content += f"\n### ğŸ“Š æ•°æ®ç»Ÿè®¡\n"
    markdown_content += f"- **æ€»äº§å“æ•°**ï¼š{len(products)}æ¬¾åˆ›æ–°äº§å“\n"
    markdown_content += f"- **æ€»ç¥¨æ•°**ï¼š{total_votes:,}ç¥¨\n"
    markdown_content += f"- **å¹³å‡ç¥¨æ•°**ï¼š{total_votes // len(products) if products else 0}ç¥¨\n"
    markdown_content += f"- **ç²¾é€‰äº§å“**ï¼š{featured_count}æ¬¾\n"
    markdown_content += f"- **çƒ­é—¨äº§å“**ï¼š{hot_products_count}æ¬¾(200+ç¥¨)\n"
    if ai_percentage > 0:
        markdown_content += f"- **AIå·¥å…·å æ¯”**ï¼š{ai_percentage}%\n"

    # æ·»åŠ è¡Œä¸šåˆ†æ
    markdown_content += f"\n{industry_analysis}\n\n"

    # æ·»åŠ äº§å“è¯¦æƒ…
    markdown_content += "## ğŸ† çƒ­é—¨äº§å“è¯¦ç»†è§£æ\n\n"

    for rank, product in enumerate(products, 1):
        markdown_content += product.to_markdown(rank)

    return markdown_content

def main():
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æ‰‹åŠ¨ç”ŸæˆæŒ‡å®šæ—¥æœŸçš„Product Huntå†…å®¹')
    parser.add_argument('--date', type=str, required=True, help='æŒ‡å®šæ—¥æœŸ (YYYY-MM-DDæ ¼å¼)')
    args = parser.parse_args()

    # éªŒè¯æ—¥æœŸæ ¼å¼
    try:
        target_datetime = datetime.strptime(args.date, '%Y-%m-%d')
        date_str = args.date
        print(f"ğŸ¯ æ‰‹åŠ¨ç”Ÿæˆæ—¥æœŸ: {date_str}")
    except ValueError:
        print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        print("ç¤ºä¾‹: python manual_generate.py --date 2025-06-18")
        return

    try:
        # è·å–æŒ‡å®šæ—¥æœŸçš„æ•°æ®
        products = fetch_product_hunt_data_for_date(date_str)

        if not products:
            print(f"âš ï¸ æœªæ‰¾åˆ° {date_str} çš„äº§å“æ•°æ®")
            return

        # ç”Ÿæˆmarkdownå†…å®¹
        markdown_content = generate_markdown_for_date(products, date_str)

        # ç¡®ä¿æ–‡ä»¶ç”Ÿæˆåˆ°é¡¹ç›®æ ¹ç›®å½•çš„dataæ–‡ä»¶å¤¹
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        data_dir = os.path.join(project_root, 'data')
        os.makedirs(data_dir, exist_ok=True)

        file_name = f"producthunt-daily-{date_str}.md"
        file_path = os.path.join(data_dir, file_name)

        with open(file_path, 'w', encoding='utf-8') as file:
            file.write(markdown_content)

        print(f"âœ… æ–‡ä»¶ {file_path} ç”ŸæˆæˆåŠŸ")

        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(file_path)
        with open(file_path, 'r', encoding='utf-8') as f:
            line_count = sum(1 for _ in f)

        print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
        print(f"ğŸ“ æ–‡ä»¶è¡Œæ•°: {line_count} è¡Œ")

        # æ£€æŸ¥voteså­—æ®µ
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            if 'votes: ' in content:
                import re
                votes_match = re.search(r'votes: (\d+)', content)
                if votes_match:
                    print(f"ğŸ¯ ç¥¨æ•°å­—æ®µ: {votes_match.group(1)}")
                else:
                    print("âš ï¸ voteså­—æ®µæ ¼å¼å¼‚å¸¸")
            else:
                print("âŒ æœªæ‰¾åˆ°voteså­—æ®µ")

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
